{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-26 14:00:19.684435 - NEW SAMPLES DETECTED; for Pool_1212 with tumour 1910418-SMP0033 & germline NaN-SMP0033; Queued for UKCloud transfer scan...\n",
      "\n",
      "2020-07-26 14:00:19.684664 - UPDATE RECORD; UKCloud transfer complete for Pool_1212 with tumour 1910418-SMP0033 & germline NaN-SMP0033 pair...\n",
      "\n",
      "2020-07-26 14:00:19.684702 - NEW SAMPLES DETECTED; for Pool_1212 with tumour 1910666-SMP0035 & germline NaN-SMP0035; Queued for UKCloud transfer scan...\n",
      "\n",
      "2020-07-26 14:00:19.684747 - UPDATE RECORD; UKCloud transfer complete for Pool_1212 with tumour 1910666-SMP0035 & germline NaN-SMP0035 pair...\n",
      "\n",
      "2020-07-26 14:00:19.685864 - NEW SAMPLES DETECTED; for Pool_1212 with tumour 1911120-SMP0038 & germline NaN-SMP0038; Queued for UKCloud transfer scan...\n",
      "\n",
      "2020-07-26 14:00:19.686099 - UPDATE RECORD; UKCloud transfer complete for Pool_1212 with tumour 1911120-SMP0038 & germline NaN-SMP0038 pair...\n",
      "\n",
      "2020-07-26 14:00:19.686168 - NEW SAMPLES DETECTED; for Pool_1212 with tumour 1912528-SMP0051 & germline NaN-SMP0051; Queued for UKCloud transfer scan...\n",
      "\n",
      "2020-07-26 14:00:19.686445 - UPDATE RECORD; UKCloud transfer complete for Pool_1212 with tumour 1912528-SMP0051 & germline NaN-SMP0051 pair...\n",
      "\n",
      "2020-07-26 14:00:19.686492 - NEW SAMPLES DETECTED; for Pool_1212 with tumour 1912529-SMP0052 & germline NaN-SMP0052; Queued for UKCloud transfer scan...\n",
      "\n",
      "2020-07-26 14:00:19.686767 - UPDATE RECORD; UKCloud transfer complete for Pool_1212 with tumour 1912529-SMP0052 & germline NaN-SMP0052 pair...\n",
      "\n",
      "2020-07-26 14:00:19.686814 - NEW SAMPLES DETECTED; for Pool_1212 with tumour 1908990-SMP0025 & germline NaN-SMP0025; Queued for UKCloud transfer scan...\n",
      "\n",
      "2020-07-26 14:00:19.686953 - UPDATE RECORD; UKCloud transfer complete for Pool_1212 with tumour 1908990-SMP0025 & germline NaN-SMP0025 pair...\n",
      "\n",
      "2020-07-26 14:00:19.687000 - NEW SAMPLES DETECTED; for Pool_1212 with tumour 1912295-SMP0048 & germline NaN-SMP0048; Queued for UKCloud transfer scan...\n",
      "\n",
      "2020-07-26 14:00:19.687071 - UPDATE RECORD; UKCloud transfer complete for Pool_1212 with tumour 1912295-SMP0048 & germline NaN-SMP0048 pair...\n",
      "\n",
      "2020-07-26 14:00:19.687134 - NEW SAMPLES DETECTED; for Pool_1212 with tumour 1912297-SMP0049 & germline NaN-SMP0049; Queued for UKCloud transfer scan...\n",
      "\n",
      "2020-07-26 14:00:19.687257 - UPDATE RECORD; UKCloud transfer complete for Pool_1212 with tumour 1912297-SMP0049 & germline NaN-SMP0049 pair...\n",
      "\n",
      "2020-07-26 14:00:19.687296 - NEW SAMPLES DETECTED; for Pool_1212 with tumour NaN-SMP0024 & germline 1908368-SMP0024; Queued for UKCloud transfer scan...\n",
      "\n",
      "2020-07-26 14:00:19.687356 - UPDATE RECORD; UKCloud transfer complete for Pool_1212 with tumour NaN-SMP0024 & germline 1908368-SMP0024 pair...\n",
      "\n",
      "2020-07-26 14:00:19.687392 - NEW SAMPLES DETECTED; for Pool_1212 with tumour NaN-SMP0006 & germline 1907015-SMP0006; Queued for UKCloud transfer scan...\n",
      "\n",
      "2020-07-26 14:00:19.687513 - UPDATE RECORD; UKCloud transfer complete for Pool_1212 with tumour NaN-SMP0006 & germline 1907015-SMP0006 pair...\n",
      "\n",
      "2020-07-26 14:00:19.687558 - NEW SAMPLES DETECTED; for Pool_1212 with tumour NaN-SMP0009 & germline 1907019-SMP0009; Queued for UKCloud transfer scan...\n",
      "\n",
      "2020-07-26 14:00:19.687668 - UPDATE RECORD; UKCloud transfer complete for Pool_1212 with tumour NaN-SMP0009 & germline 1907019-SMP0009 pair...\n",
      "\n",
      "2020-07-26 14:00:19.687706 - NEW SAMPLES DETECTED; for Pool_1212 with tumour NaN-SMP0011 & germline 1907022-SMP0011; Queued for UKCloud transfer scan...\n",
      "\n",
      "2020-07-26 14:00:19.687778 - UPDATE RECORD; UKCloud transfer complete for Pool_1212 with tumour NaN-SMP0011 & germline 1907022-SMP0011 pair...\n",
      "\n",
      "2020-07-26 14:00:19.687821 - NEW SAMPLES DETECTED; for Pool_1212 with tumour NaN-SMP0012 & germline 1907025-SMP0012; Queued for UKCloud transfer scan...\n",
      "\n",
      "2020-07-26 14:00:19.687997 - UPDATE RECORD; UKCloud transfer complete for Pool_1212 with tumour NaN-SMP0012 & germline 1907025-SMP0012 pair...\n",
      "\n",
      "2020-07-26 14:00:19.688047 - NEW SAMPLES DETECTED; for Pool_1212 with tumour NaN-SMP0005 & germline 1906811-SMP0005; Queued for UKCloud transfer scan...\n",
      "\n",
      "2020-07-26 14:00:19.688179 - UPDATE RECORD; UKCloud transfer complete for Pool_1212 with tumour NaN-SMP0005 & germline 1906811-SMP0005 pair...\n",
      "\n",
      "2020-07-26 14:00:19.688218 - NEW SAMPLES DETECTED; for Pool_1212 with tumour NaN-SMP0007 & germline 1906814-SMP0007; Queued for UKCloud transfer scan...\n",
      "\n",
      "2020-07-26 14:00:19.688255 - UPDATE RECORD; UKCloud transfer complete for Pool_1212 with tumour NaN-SMP0007 & germline 1906814-SMP0007 pair...\n",
      "\n",
      "2020-07-26 14:00:19.688290 - NEW SAMPLES DETECTED; for Pool_1212 with tumour NaN-SMP0035 & germline 1910665-SMP0035; Queued for UKCloud transfer scan...\n",
      "\n",
      "2020-07-26 14:00:19.688393 - UPDATE RECORD; UKCloud transfer complete for Pool_1212 with tumour NaN-SMP0035 & germline 1910665-SMP0035 pair...\n",
      "\n",
      "2020-07-26 14:00:19.688428 - NEW SAMPLES DETECTED; for Pool_1212 with tumour NaN-SMP0048 & germline 1911997-SMP0048; Queued for UKCloud transfer scan...\n",
      "\n",
      "2020-07-26 14:00:19.688530 - UPDATE RECORD; UKCloud transfer complete for Pool_1212 with tumour NaN-SMP0048 & germline 1911997-SMP0048 pair...\n",
      "\n",
      "2020-07-26 14:00:19.688565 - NEW SAMPLES DETECTED; for Pool_1212 with tumour NaN-SMP0050 & germline 1912298-SMP0050; Queued for UKCloud transfer scan...\n",
      "\n",
      "2020-07-26 14:00:19.688684 - UPDATE RECORD; UKCloud transfer complete for Pool_1212 with tumour NaN-SMP0050 & germline 1912298-SMP0050 pair...\n",
      "\n",
      "2020-07-26 14:00:19.688738 - NEW SAMPLES DETECTED; for Pool_1212 with tumour NaN-SMP0052 & germline 191227-SMP0052; Queued for UKCloud transfer scan...\n",
      "\n",
      "2020-07-26 14:00:19.688875 - UPDATE RECORD; UKCloud transfer complete for Pool_1212 with tumour NaN-SMP0052 & germline 191227-SMP0052 pair...\n",
      "\n",
      "2020-07-26 14:00:19.688975 - NEW SAMPLES DETECTED; for Pool_1212 with tumour NaN-SMP0051 & germline 1912525-SMP0051; Queued for UKCloud transfer scan...\n",
      "\n",
      "2020-07-26 14:00:19.689037 - UPDATE RECORD; UKCloud transfer complete for Pool_1212 with tumour NaN-SMP0051 & germline 1912525-SMP0051 pair...\n",
      "\n",
      "-------------------------DONE-------------------------\n"
     ]
    }
   ],
   "source": [
    "#!/bin/env python\n",
    "\n",
    "#################\n",
    "## Description ##\n",
    "#################\n",
    "#- Python script for automation of data transfer to UKCloud by targeting\n",
    "# specific samples for a given project supplied through the YAML\n",
    "# config file. Originally developed for the StratMed PAEDs project.\n",
    "\n",
    "###########\n",
    "## Input ##\n",
    "###########\n",
    "# Input given at call of script.\n",
    "# - YAML config file\n",
    "\n",
    "#####################\n",
    "## Related scripts ##\n",
    "#####################\n",
    "#- transfer_script.sh => Collects data for a given sample and uploads to\n",
    "# UKCloud together with transfer log generated by\n",
    "# this script.\n",
    "\n",
    "#By: Sabri Jamal\n",
    "#Date: 26/07-2020\n",
    "#Version: 2.0\n",
    "\n",
    "##------------------------------------------------------------------------------------------------------------------------------------------##\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import getopt\n",
    "import pdb\n",
    "import re\n",
    "import subprocess as subp\n",
    "import datetime\n",
    "import yaml\n",
    "#import pdb;pdb.set_trace()\n",
    "\n",
    "class UKCloud(object):\n",
    "    ##Class attribute\n",
    "    config = None\n",
    "    transfer_log_header = \"Pool\\tTrial_ID\\tTumour\\tBaseline\\tType\\tCheck1\\tDate_Check1\\tCheck2\\tGermline\\tDate_Germline\\tUKCloud\\tDate_UKCloud\\n\"\n",
    "    \n",
    "    ##Available data types for transfer\n",
    "    panel_relapse_dt = \"Panel-Relapse\"\n",
    "    exome_dt = \"Exome\"\n",
    "    panel_primary_dt = \"Panel-Primary\"\n",
    "    cell_free_dt = \"ctDNA\"\n",
    "    low_copy_whole_genome_dt = \"lcWGS\"\n",
    "    rna_capture_dt = \"RNA-capture\" #Can't be implemented until decision on BaseSpace\n",
    "        \n",
    "    #Transfer log header labels\n",
    "    t_log_header_pool = [transfer_log_header.split(\"\\t\")[0], 0]\n",
    "    t_log_header_trialID = [transfer_log_header.split(\"\\t\")[1], 1]\n",
    "    t_log_header_tumour = [transfer_log_header.split(\"\\t\")[2], 2]\n",
    "    t_log_header_baseline = [transfer_log_header.split(\"\\t\")[3], 3]\n",
    "    t_log_header_type = [transfer_log_header.split(\"\\t\")[4], 4]\n",
    "    t_log_header_check1 = [transfer_log_header.split(\"\\t\")[5], 5]\n",
    "    t_log_header_date_check1 = [transfer_log_header.split(\"\\t\")[6], 6]\n",
    "    t_log_header_check2 = [transfer_log_header.split(\"\\t\")[7], 7]\n",
    "    t_log_header_germline = [transfer_log_header.split(\"\\t\")[8], 8]\n",
    "    t_log_header_date_germline = [transfer_log_header.split(\"\\t\")[9], 9]\n",
    "    t_log_header_ukcloud = [transfer_log_header.split(\"\\t\")[10], 10]\n",
    "    t_log_header_date_ukcloud = [transfer_log_header.split(\"\\t\")[11], 11]\n",
    "    \n",
    "    def __init__(self, config_file):\n",
    "        #Instanciation vars\n",
    "        self.config_file = config_file\n",
    "        self.config = self.load_config(self.config_file)\n",
    "\n",
    "    #Reads the yaml config file\n",
    "    def load_config(self, config_file):\n",
    "        with open(config_file, \"r\") as IN_config:\n",
    "            UKCloud.config = yaml.load(IN_config)\n",
    "            \n",
    "    def write_dict2line(self, line_dict):\n",
    "        tsv_line = \"\"\n",
    "        for index, transfer_log_header_item in enumerate(re.split(\"\\t\",self.transfer_log_header)):\n",
    "            if(index == 0):\n",
    "                tsv_line = str(line_dict[transfer_log_header_item])\n",
    "            else:\n",
    "                tsv_line = tsv_line + \"\\t\" + str(line_dict[transfer_log_header_item])\n",
    "\n",
    "        return(tsv_line)\n",
    "             \n",
    "    #Function load succesfully transferred samples into dict. Dict is used\n",
    "    #to determine which samples to skip within func write_dict_to_file\n",
    "    def load_file_as_3lvl_nested_dict(self, succesful_transfers_file):\n",
    "        samples_previously_queued_dict = {}\n",
    "        header = True\n",
    "\n",
    "        #Parse samples succesfully transferred to UKCloud\n",
    "        try:\n",
    "            with open(succesful_transfers_file, \"r\") as succ_transf_pool_IN:\n",
    "                for line in succ_transf_pool_IN:\n",
    "                    \n",
    "                    #Skip header\n",
    "                    if(header):\n",
    "                        header = False\n",
    "                        continue\n",
    "                    \n",
    "                    sample_pair_dict = {} #reset\n",
    "                    line = line.rstrip()\n",
    "                    match = re.split(\"\\t\", line)\n",
    "                    pool_id = match[0]\n",
    "                    trial_id = match[1]\n",
    "                    tumour = match[2]\n",
    "                    germline = match[3]\n",
    "                    sample_pair_dict = {\"tumour\": tumour, \"germline\": germline} #germline = NaN if tumour only\n",
    "                    \n",
    "                    key = \"{pool}:{tumour}:{germline}\".format(pool=pool_id, tumour=tumour, germline=germline)\n",
    "\n",
    "                    if(pool_id not in samples_previously_queued_dict):\n",
    "                        samples_previously_queued_dict[pool_id] = {key : sample_pair_dict}\n",
    "                    elif(key not in samples_previously_queued_dict[pool_id]):\n",
    "                        samples_previously_queued_dict[pool_id][key] = sample_pair_dict\n",
    "                    if(pool_id in samples_previously_queued_dict and tumour not in samples_previously_queued_dict[pool_id][key][\"tumour\"]):\n",
    "                        samples_previously_queued_dict[pool_id][key][\"tumour\"] = tumour\n",
    "                    if(pool_id in samples_previously_queued_dict and germline not in samples_previously_queued_dict[pool_id][key][\"germline\"]):\n",
    "                        samples_previously_queued_dict[pool_id][key][\"germline\"] = germline\n",
    "        except FileNotFoundError:\n",
    "            prompt = \"{ts} - WARNING; Log file tracking succesfuly transferred samples not found, no samples will be skipped.\".format(ts=str(datetime.datetime.now()))\n",
    "            print(prompt)\n",
    "\n",
    "        return(samples_previously_queued_dict)\n",
    "\n",
    "    ##Parses all sample sheets selectively choosing sample sheets containing\n",
    "    # StratMed PAED (SMP) suffix using a global grep for primary filtering\n",
    "    def parse_sample_sheets(self):\n",
    "\n",
    "        ##Instantiate static variables\n",
    "        sample_sheet_suffix = UKCloud.config['sample-sheet']['suffix']\n",
    "        sample_sheet_dir = UKCloud.config['file_system_objects']['sample_sheet_path']\n",
    "\n",
    "        glob_ss_dict = {} #holds all sample sheets (nested)\n",
    "\n",
    "        ##Body\n",
    "        direc_listing = os.listdir(sample_sheet_dir)\n",
    "        for fso in direc_listing:\n",
    "\n",
    "            #Skip if reading non target sample sheets\n",
    "            if(fso.find(\"secondaryanalyses\") >= 0 or fso.find(\"demultiplex\") >= 0):\n",
    "                continue\n",
    "\n",
    "            fso_abs_path = os.path.join(sample_sheet_dir, fso)\n",
    "            ss_dict = {} #holds one sample sheet stored in nested dict glob_ss_dict\n",
    "\n",
    "            if( os.path.isdir(fso_abs_path) ):\n",
    "                continue\n",
    "\n",
    "            match = re.search(sample_sheet_suffix, fso)\n",
    "            if(match):\n",
    "\n",
    "                ##Check sample sheet contains SMPaed sample\n",
    "                cmd = 'grep \"SMP\" '\n",
    "                cmd = cmd + fso_abs_path\n",
    "                grep = subp.Popen(cmd, stdout=subp.PIPE, shell=True)\n",
    "\n",
    "                #Decode byte & parse sample sheet\n",
    "                if(grep.stdout.readline().decode(\"UTF-8\") != \"\"):\n",
    "\n",
    "                    with open(fso_abs_path, \"r\") as ss_IN:\n",
    "                        meta = True\n",
    "                        header = True\n",
    "                        ss_dict = {}\n",
    "\n",
    "                        for line in ss_IN:\n",
    "\n",
    "                            #Skip empty lines\n",
    "                            if(not line.strip()):\n",
    "                                continue\n",
    "\n",
    "                            line = line.rstrip()\n",
    "\n",
    "                            #TODO# Future improvement - To add logic to handle\n",
    "                            # hashed out lines. See top head of script for more\n",
    "                            # details\n",
    "                            # sample sheet\n",
    "                            #if(line.startswith(\"#Pool\")):\n",
    "                            #\tmeta = False\n",
    "                            #elif(line.startswith(\"#\")):\n",
    "                            #\tmeta = True\n",
    "                            #else:\n",
    "                            #\tmeta = False\n",
    "\n",
    "                            #Logic to detect meta data or commented lines in\n",
    "                            #sample sheet\n",
    "                            if(line.startswith(\"#\")):\n",
    "                                meta = True\n",
    "                            else:\n",
    "                                meta = False\n",
    "\n",
    "                            #Parse non meta data\n",
    "                            if(meta):\n",
    "                                continue\n",
    "                            else:\n",
    "                                #Store header data\n",
    "                                if(header):\n",
    "                                    header_array = re.split(\",\", line)\n",
    "                                    header = False\n",
    "                                else:\n",
    "                                    data_list = re.split(\",\", line)\n",
    "\n",
    "                                #Store per sample sheet data in dict\n",
    "                                for i in range(0, len(header_array)):\n",
    "                                    ss_header = header_array[i].lower()\n",
    "                                    if(ss_header not in ss_dict):\n",
    "                                        ss_dict[ss_header] = []\n",
    "                                    else:\n",
    "                                        prev_list = ss_dict[ss_header]\n",
    "\n",
    "                                        ##Debugging\n",
    "                                        try:\n",
    "                                            prev_list.append(data_list[i])\n",
    "                                            ss_dict[ss_header] = prev_list\n",
    "                                        except IndexError as e:\n",
    "                                            prompt=\"{ts} - WARNING; Number of columns don't match with the data for sample sheet {fso}\".format(fso=fso, ts=str(datetime.datetime.now()))\n",
    "                                            print(prompt)\n",
    "\n",
    "\n",
    "            #Store each sample sheet dict in nested dict\n",
    "            if(fso not in glob_ss_dict):\n",
    "                glob_ss_dict[fso] = ss_dict\n",
    "            else:\n",
    "                prompt = \"{ts} - WARNING; Duplicate sample sheet found for {fso}\".format(fso=fso, ts=str(datetime.datetime.now()))\n",
    "                print(prompt)\n",
    "\n",
    "        return (glob_ss_dict)\n",
    "\n",
    "\n",
    "    ##Writes the returned dict object from parse_sample_sheets to a file\n",
    "    # containing new samples to be scanned for sending.\n",
    "    def write_dict_to_file(self, glob_ss_dict):\n",
    "        ##Instantiate static variables\n",
    "        allowed_panels = [ UKCloud.config['panels']['paediatric'], UKCloud.config['panels']['exome'] ]\n",
    "        bed_col = UKCloud.config['sample-sheet']['bedfile_col']\n",
    "        seq_pool_id = [ UKCloud.config['sample-sheet']['pool_id_col'] ]\n",
    "        sample_name_col = [ UKCloud.config['sample-sheet']['sample_name_col'] ]\n",
    "        sample_pair_name_col = [ UKCloud.config['sample-sheet']['sample_pair_name_col'] ]\n",
    "        wild_card_col = [ UKCloud.config['sample-sheet']['wild_card_col'] ]\n",
    "        sample_type = [ UKCloud.config['sample-sheet']['sample_type'] ]\n",
    "        ready_to_send = UKCloud.config['log_files']['transfer_log']\n",
    "        output_path = UKCloud.config['file_system_objects']['logfile_dest_path']\n",
    "\n",
    "        ##Instantiate dynamic variables\n",
    "        pool_id_list = []\n",
    "        moldx_sample_t_list = []\n",
    "        moldx_sample_b_list = []\n",
    "        trial_id_list = []\n",
    "        data_type_list = []\n",
    "        \n",
    "        ##Set full paths for log files\n",
    "        ready_log_abs_path = os.path.join(output_path, ready_to_send)\n",
    "\n",
    "        ##Store all pools previously transferred to UKCloud\n",
    "        if(os.path.exists(ready_log_abs_path)):\n",
    "            samples_previously_queued_dict = self.load_file_as_3lvl_nested_dict(ready_log_abs_path)\n",
    "        else:\n",
    "            prompt = \"{ts} - WARNING; Log file tracking succesfuly transferred samples not found, no samples will be skipped.\".format(ts=str(datetime.datetime.now()))\n",
    "            print(prompt)\n",
    "\n",
    "        ##Create output file with header if doesn't exist\n",
    "        if(not os.path.exists(ready_log_abs_path)):            \n",
    "            with open( os.path.join(ready_log_abs_path), \"w\") as ready_IN:\n",
    "                ready_IN.write(self.transfer_log_header)\n",
    "\n",
    "        ##Loop to access sample sheet dict for each pool\n",
    "        for pool, ss_dict in glob_ss_dict.items():\n",
    "\n",
    "            ##Skip empty sample sheets that were picked up to noto have SMPAEDs\n",
    "            #sample through global\n",
    "            if(len(ss_dict) == 0):\n",
    "                continue\n",
    "\n",
    "            #Skip germline sample sheets ending with suffix G\n",
    "            if(pool.split(\".\")[0].endswith(\"G\")):\n",
    "                continue\n",
    "\n",
    "            ss_dict_samp_name_list = ss_dict[sample_name_col[0]].copy()\n",
    "\n",
    "            #Reset variables\n",
    "            del_tumour_inds = []\n",
    "            del_germline_inds = []\n",
    "            del_inds = []\n",
    "            target_ind = []\n",
    "            tumour_ind = []\n",
    "\n",
    "            #Extract pool id from sample sheet\n",
    "            pool_id = pool.split(\".\")[0]\n",
    "            \n",
    "            ## Logic to skip samples already transferred to the cloud\n",
    "            #=========================================================\n",
    "            try:\n",
    "                # Controls samples to only be logged once in transfer log file\n",
    "                if(pool_id in samples_previously_queued_dict.keys()):\n",
    "                    #Compare tumour/germline id from file with dict and delete entry\n",
    "                    # if exists\n",
    "                    for key, sample_pair_dict in samples_previously_queued_dict[pool_id].items():\n",
    "                        tumour = sample_pair_dict[\"tumour\"]\n",
    "                        germline = sample_pair_dict[\"germline\"]\n",
    "                        for sample in ss_dict_samp_name_list:\n",
    "                            if( sample.find(tumour) >= 0):\n",
    "                                del_tumour_inds.append(ss_dict_samp_name_list.index(sample))\n",
    "\n",
    "                            if( sample.find(germline) >= 0):\n",
    "                                del_germline_inds.append(ss_dict_samp_name_list.index(sample))\n",
    "\n",
    "                    ##Delete entries for tumour & germline samples succesfuly transferred\n",
    "                    if( len(set(del_tumour_inds) & set(del_germline_inds)) == 0 ):\n",
    "                        del_inds = del_tumour_inds + del_germline_inds\n",
    "                        for column_key, value_list in ss_dict.items():\n",
    "                            for index in sorted(del_inds, reverse=True):\n",
    "\n",
    "                                #Empty columns are skipped in sample sheet as they are expected\n",
    "                                # to be trailing commas in csv doc\n",
    "                                if(column_key == \"\"):\n",
    "                                    continue\n",
    "\n",
    "                                del ss_dict[column_key][index]\n",
    "                    else:\n",
    "                        prompt = \"{ts} - WARNING; Possible ambigous match was found when searching moldx ID for tumour or germline in sample sheet {ss_sheet}. Likely reason; stumbled upon tumour only analysis where sample_id and gatk_grp column have same value\".format(ss_sheet=pool, ts=str(datetime.datetime.now()))\n",
    "                        print(prompt)\n",
    "\n",
    "            except UnboundLocalError:\n",
    "                prompt = \"{ts} - WARNING; Log file tracking succesfuly transferred samples not found, no samples will be skipped.\".format(ts=str(datetime.datetime.now()))\n",
    "                print(prompt)\n",
    "\n",
    "\n",
    "            ## New samples are screened for eligibility\n",
    "            #============================================\n",
    "            ##Check that analysed on PAEDs panel to target only SMPaeds sample\n",
    "            #second level of security\n",
    "            panel1 = set([allowed_panels[0]]) & set(ss_dict[bed_col]) #PAED\n",
    "            panel2 = set([allowed_panels[1]]) & set(ss_dict[bed_col]) #Exome\n",
    "            comb_cond = len(panel1) + len(panel2)\n",
    "            if(comb_cond == 0):\n",
    "                continue #skip if ss does not contain any of the targets\n",
    "            else:\n",
    "                #Store header as list by converting from dict.key obj to\n",
    "                #list obj to access index\n",
    "                header_array = [header for header in ss_dict.keys()]\n",
    "\n",
    "                ##Locate indexes for target samples containing right trial ID\n",
    "                target_ind = [i for i, sample in enumerate(ss_dict[sample_name_col[0]]) if(re.search(\"-SMP\\d+-\", sample))]\n",
    "\n",
    "                ##Locate indexes for all tumour samples\n",
    "                # NOTE! Exomes can be sequenced with either only baseline or tumour condition can't wait for match!\n",
    "                tumour_ind = [i for i, samp_type in enumerate(ss_dict[sample_type[0]]) if(samp_type.lower() == \"tumour\") ]\n",
    "\n",
    "                #Select all indexes on PAEDs panel\n",
    "                panel_ind = [i for i, panel_bed in enumerate(ss_dict[bed_col]) if(panel_bed.lower() == allowed_panels[0].lower()) ]\n",
    "\n",
    "                ##Locate indexes for all exomes\n",
    "                # NOTE! Add case specific name for exome and keep current set up in place\n",
    "                # NOTE! One example of location to add conditional to skip sample sheet with 'other' column set to qc_run (catch exception though!!)\n",
    "                exome_ind = [i for i, exome_bed in enumerate(ss_dict[bed_col]) if(exome_bed.lower() == allowed_panels[1].lower()) ]\n",
    "\n",
    "                ##Select only tumour samples on PAED panel i.e. find intersect from sample\n",
    "                #sheet to avoid duplicates in ready to transfer file\n",
    "                target_ind_panel = set(target_ind) & set(panel_ind)\n",
    "                target_ind_panel = set(target_ind_panel) & set(tumour_ind)\n",
    "\n",
    "                #Select exomes indexes\n",
    "                target_ind_exome = set(target_ind) & set(exome_ind)\n",
    "\n",
    "            ## Fetch samples (PANEL relapse) eligible to be queued \n",
    "            #  (to be written to transfer log) for transfer check.\n",
    "            #======================================================\n",
    "            if(len(target_ind_panel) != 0):\n",
    "                for ind in target_ind_panel:\n",
    "                    sample_moldx_t = ss_dict.get(sample_name_col[0])[ind]\n",
    "                    sample_moldx_b = ss_dict.get(sample_pair_name_col[0])[ind]\n",
    "                    sample_trial_id =  ss_dict.get(sample_name_col[0])[ind]\n",
    "                    match_moldx_t = re.search(\"(\\d+)-SMP\\d+\", sample_moldx_t)\n",
    "                    match_moldx_b = re.search(\"(\\d+)-SMP\\d+\", sample_moldx_b)\n",
    "                    match_trial_id = re.search(\"\\d+-(SMP\\d+)\",sample_trial_id)\n",
    "\n",
    "                    if(match_moldx_t and match_moldx_b and match_trial_id):\n",
    "                        pool_id_list.append(ss_dict.get(seq_pool_id[0])[ind])\n",
    "                        moldx_sample_t_list.append(match_moldx_t.group(1))\n",
    "                        moldx_sample_b_list.append(match_moldx_b.group(1))\n",
    "                        trial_id_list.append(match_trial_id.group(1))\n",
    "                        data_type_list.append(self.panel_relapse_dt)\n",
    "                    else:\n",
    "                        prompt=\"{ts} - WARNING; Tumour {tumour}, germline {germline} or trial id {trial_id} does not match target project name structure\".format(tumour=sample_moldx_t, germline=sample_moldx_b, trial_id=sample_trial_id, ts=str(datetime.datetime.now()))\n",
    "\n",
    "            ## Fetch samples (EXOME) eligible to be queued \n",
    "            #  (to be written to transfer log) for transfer check.\n",
    "            #======================================================\n",
    "            if(len(target_ind_exome) != 0):\n",
    "                for ind in target_ind_exome:\n",
    "\n",
    "                    ##Ignore run if quality control test\n",
    "                    try:\n",
    "                        sample_check_if_qc = ss_dict.get(wild_card_col[0])[ind]\n",
    "                        if(sample_check_if_qc == \"qc_run\"):\n",
    "                            continue\n",
    "                        \n",
    "                    except TypeError:\n",
    "                        sample_check_if_qc = None #column does not exist\n",
    "\n",
    "                    sample_moldx_generic = ss_dict.get(sample_name_col[0])[ind]\n",
    "                    sample_tag_generic = ss_dict.get(sample_type[0])[ind]\n",
    "                    match_moldx_generic = re.search(\"(\\d+)-SMP\\d+\", sample_moldx_generic)\n",
    "                    match_trial_id = re.search(\"\\d+-(SMP\\d+)\",sample_moldx_generic)\n",
    "                    \n",
    "                    if(match_moldx_generic and match_trial_id):\n",
    "                        pool_id_list.append(ss_dict.get(seq_pool_id[0])[ind])\n",
    "                        trial_id_list.append(match_trial_id.group(1))\n",
    "\n",
    "                        # NOTE! This is where generic name needs to be tested if either normal or tumour by looking at tag column in SS\n",
    "                        if(sample_tag_generic.lower() == \"tumour\" or sample_tag_generic.lower() == \"tumor\"):\n",
    "                            moldx_sample_t_list.append(match_moldx_generic.group(1))\n",
    "                            moldx_sample_b_list.append(\"NaN\")\n",
    "                            data_type_list.append(self.exome_dt)\n",
    "\n",
    "                        elif(sample_tag_generic.lower() == \"normal\"):\n",
    "                            moldx_sample_t_list.append(\"NaN\")\n",
    "                            moldx_sample_b_list.append(match_moldx_generic.group(1))\n",
    "                            data_type_list.append(self.exome_dt)\n",
    "                    else:\n",
    "                        prompt=\"{ts} - WARNING; Exome sample name {tumour} or trial id {trial_id} does not match target project name structure\".format(tumour=sample_moldx_t, trial_id=sample_trial_id, ts=str(datetime.datetime.now()))\n",
    "\n",
    "        ## Queue eligible samples (write to transfer log) to be scanned in \n",
    "        #  transfer_UKCloud func to determine if eligible for transfer to cloud \n",
    "        #=======================================================================\n",
    "        with open( os.path.join(ready_log_abs_path), \"a\") as ready_IN:\n",
    "            equal_tot_length = len(pool_id_list) #used as template as all should be equal sized\n",
    "            if( len(pool_id_list) == len(moldx_sample_t_list) == len(moldx_sample_b_list) == len(trial_id_list) and equal_tot_length > 0 ):\n",
    "                for i in range(0, equal_tot_length):\n",
    "                    ready_IN.write(pool_id_list[i] + \"\\t\" + trial_id_list[i] + \"\\t\" + moldx_sample_t_list[i] + \"\\t\" + moldx_sample_b_list[i] + \"\\t\" + data_type_list[i] + \"\\n\")\n",
    "\n",
    "    ##Function to check if samples are expected to have check 1 and check 2\n",
    "    # for both somatic and germline analysis for transfer eligibility.\n",
    "    # Returns updated line as dictionary.\n",
    "    def full_check_ck1ck2germ(self, line_dict, match):\n",
    "        \n",
    "        ##Instantiate static variables\n",
    "        analysis_folder_root_path = UKCloud.config['file_system_objects']['analysis_folder_root_path']\n",
    "        analysis_reports_folder = UKCloud.config['file_system_objects']['analysis_reports_folder']\n",
    "        uk_cloud_transfer_script = UKCloud.config['file_system_objects']['uk_cloud_transfer_script']\n",
    "        germline_pool_suffix = \"G\"\n",
    "        \n",
    "        #Set full sample name & absolute path to reports\n",
    "        pool_id = match[self.t_log_header_pool[1]]\n",
    "        sample_id_t = match[self.t_log_header_tumour[1]]\n",
    "        sample_id_b = match[self.t_log_header_baseline[1]]\n",
    "        trial_id = match[self.t_log_header_trialID[1]]\n",
    "        full_sample_name_t = \"{sample_t}-{trialID}\".format(sample_t=sample_id_t, trialID=trial_id) \n",
    "        full_sample_name_b = \"{sample_b}-{trialID}\".format(sample_b=sample_id_b, trialID=trial_id)\n",
    "        germline_pool_id = \"{pool_id}G\".format(pool_id=pool_id)\n",
    "        report_abs_path_somatic = os.path.join(analysis_folder_root_path, pool_id, analysis_reports_folder)\n",
    "        report_abs_path_germline = os.path.join(analysis_folder_root_path, germline_pool_id, analysis_reports_folder)\n",
    "\n",
    "\n",
    "        ##Attempt to set dyanmic variables related to samples checked or\n",
    "        # transferred. If values are set to NaN set the boolean value to false\n",
    "        ## CHECK 1\n",
    "        # ========\n",
    "        try:\n",
    "            check1 = match[self.t_log_header_check1[1]]\n",
    "            if(check1 == \"NaN\"):\n",
    "                check1 = False\n",
    "            elif(check1 == \"True\"):\n",
    "                check1 = True                    \n",
    "\n",
    "                line_dict[self.t_log_header_check1[0]] = match[self.t_log_header_check1[1]]\n",
    "                line_dict[self.t_log_header_date_check1[0]] = match[self.t_log_header_date_check1[1]]   \n",
    "        except:\n",
    "            prompt = \"{ts} - NEW SAMPLES DETECTED; {pool_id} with tumour {tumour} & germline {germline}; Queued for UPDATE RECORD; Somatic checker 1 scan...\\n\".format(ts=str(datetime.datetime.now()), tumour=full_sample_name_t,germline=full_sample_name_b, pool_id=pool_id)\n",
    "            print(prompt)\n",
    "            check1 = False\n",
    "\n",
    "        ## CHECK 2\n",
    "        #==========\n",
    "        try:\n",
    "            check2 = match[self.t_log_header_check2[1]]\n",
    "            if(check2 == \"NaN\"):\n",
    "                check2 = False\n",
    "            elif(check2 == \"True\"):\n",
    "                check2 = True\n",
    "\n",
    "            line_dict[self.t_log_header_check2[0]] = match[self.t_log_header_check2[1]]\n",
    "        except:\n",
    "            prompt = \"{ts} - NEW SAMPLES DETECTED; {pool_id} with tumour {tumour} & germline {germline}; Queued for UPDATE RECORD; Somatic checker 2 scan...\\n\".format(ts=str(datetime.datetime.now()), tumour=full_sample_name_t,germline=full_sample_name_b, pool_id=pool_id)\n",
    "            print(prompt)\n",
    "            check2 = False\n",
    "\n",
    "        ## GERMLINE\n",
    "        #===========\n",
    "        try:\n",
    "            germline = match[self.t_log_header_germline[1]]\n",
    "            if(germline == \"NaN\"):\n",
    "                germline = False\n",
    "            elif(germline == \"True\"):\n",
    "                germline = True\n",
    "\n",
    "            line_dict[self.t_log_header_germline[0]] = match[self.t_log_header_germline[1]]\n",
    "            line_dict[self.t_log_header_date_germline[0]] = match[self.t_log_header_date_germline[1]]\n",
    "        except:\n",
    "            prompt = \"{ts} - NEW SAMPLES DETECTED; for {pool_id} with germline {germline}; Queued for germline check scan...\\n\".format(ts=str(datetime.datetime.now()), germline=full_sample_name_b, pool_id=pool_id)\n",
    "            print(prompt)\n",
    "            germline = False\n",
    "\n",
    "        ## UKCloud\n",
    "        #==========\n",
    "        try:\n",
    "            uk_cloud = match[self.t_log_header_ukcloud[1]]\n",
    "            if(uk_cloud == \"NaN\"):\n",
    "                uk_cloud = False\n",
    "            elif(uk_cloud == \"True\"):\n",
    "                uk_cloud = True\n",
    "\n",
    "            line_dict[self.t_log_header_ukcloud[0]] = match[self.t_log_header_ukcloud[1]]\n",
    "            line_dict[self.t_log_header_date_ukcloud[0]] = match[self.t_log_header_date_ukcloud[1]]\n",
    "\n",
    "        except:\n",
    "            prompt = \"{ts} - NEW SAMPLES DETECTED; for {pool_id} with tumour {tumour} & germline {germline}; Queued for UKCloud transfer scan...\\n\".format(ts=str(datetime.datetime.now()), tumour=full_sample_name_t,germline=full_sample_name_b, pool_id=pool_id)\n",
    "            print(prompt)\n",
    "            uk_cloud = False\n",
    "\n",
    "        ## Check patient reports and eligibility to transfer data\n",
    "        #  based on logged info in transfer log\n",
    "        #========================================================\n",
    "        if(not check1):\n",
    "            c1_bol = False\n",
    "            regex_cmd_c1 = full_sample_name_t + \".*\\.patient\\.report\\.\\w+\"\n",
    "            regex_cmd_pr = full_sample_name_t + \".*\\.patient\\.report\\.tsv\"\n",
    "\n",
    "            if(os.path.exists(report_abs_path_somatic)):\n",
    "                for report in os.listdir(report_abs_path_somatic):\n",
    "\n",
    "                    patient_reprt = re.search(regex_cmd_pr, report)\n",
    "                    c1 = re.search(regex_cmd_c1, report)\n",
    "\n",
    "                    #Ensure that that regex only matches checker 1 and not patient.report.tsv\n",
    "                    if(not patient_reprt and c1):\n",
    "                        c1_bol = True\n",
    "\n",
    "                if(c1_bol):\n",
    "                    check1 = True\n",
    "                    line_dict[self.t_log_header_check1[0]] = \"True\"\n",
    "                    line_dict[self.t_log_header_date_check1[0]] = str(datetime.datetime.now().date())\n",
    "                    prompt = \"{ts} - UPDATE RECORD; Somatic checker 1 complete for {pool_id} with tumour {tumour} & germline {germline} pair...\\n\".format(ts=str(datetime.datetime.now()), tumour=full_sample_name_t,germline=full_sample_name_b, pool_id=pool_id)\n",
    "                    print(prompt)\n",
    "                else:\n",
    "                    line_dict[self.t_log_header_check1[0]] = \"NaN\"\n",
    "                    line_dict[self.t_log_header_date_check1[0]] = \"NaN\"\n",
    "                    prompt = \"{ts} - UPDATE RECORD; Somatic checker 1 NOT complete for {pool_id} with tumour {tumour} & germline {germline} pair...\\n\".format(ts=str(datetime.datetime.now()), tumour=full_sample_name_t,germline=full_sample_name_b, pool_id=pool_id)\n",
    "                    print(prompt)\n",
    "\n",
    "        if(not check2):\n",
    "            c1_bol = False\n",
    "            c2_bol = False\n",
    "            regex_cmd_c1 = full_sample_name_t + \".*\\.patient\\.report\\.\\w+\"\n",
    "            regex_cmd_c2 = full_sample_name_t + \".*\\.patient\\.report\"\n",
    "\n",
    "            if(os.path.exists(report_abs_path_somatic)):\n",
    "                for report in os.listdir(report_abs_path_somatic):\n",
    "\n",
    "                    c1 = re.search(regex_cmd_c1, report)\n",
    "                    c2 = re.search(regex_cmd_c2, report)\n",
    "\n",
    "                    if(not c1 and c2):\n",
    "                        c2_bol = True\n",
    "\n",
    "                if(c2_bol):\n",
    "                    check2 = True\n",
    "                    line_dict[self.t_log_header_check2[0]] = \"True\"\n",
    "                    prompt = \"{ts} - UPDATE RECORD; Somatic checker 2 complete for {pool_id} with tumour {tumour} & germline {germline} pair...\\n\".format(ts=str(datetime.datetime.now()), tumour=full_sample_name_t,germline=full_sample_name_b, pool_id=pool_id)\n",
    "                    print(prompt)\n",
    "                else:\n",
    "                    line_dict[self.t_log_header_check2[0]] = \"NaN\"\n",
    "                    prompt = \"{ts} - UPDATE RECORD; Somatic checker 2 NOT complete for {pool_id} with tumour {tumour} & germline {germline} pair...\\n\".format(ts=str(datetime.datetime.now()), tumour=full_sample_name_t,germline=full_sample_name_b, pool_id=pool_id)\n",
    "                    print(prompt)\n",
    "\n",
    "        if(not germline):\n",
    "            regex_cmd_c1 = full_sample_name_b + \".*\\.patient\\.report\\.\\w+\"\n",
    "            regex_cmd_c2 = full_sample_name_b + \".*\\.patient\\.report\"\n",
    "            regex_cmd_pr = full_sample_name_b + \".*\\.patient\\.report\\.tsv\"\n",
    "            c1_bol = False\n",
    "            c2_bol = False\n",
    "\n",
    "            try:\n",
    "                for report in os.listdir(report_abs_path_germline):\n",
    "                    patient_reprt = re.search(regex_cmd_pr, report)\n",
    "                    c1 = re.search(regex_cmd_c1, report)\n",
    "                    c2 = re.search(regex_cmd_c2, report)\n",
    "\n",
    "                    if(c1 and not patient_reprt):\n",
    "                        c1_bol = True\n",
    "\n",
    "                    if(c2 and not c1):\n",
    "                        c2_bol = True\n",
    "\n",
    "                if(c1_bol and c2_bol):\n",
    "                    germline = True\n",
    "                    line_dict[self.t_log_header_germline[0]] = \"True\"\n",
    "                    line_dict[self.t_log_header_date_germline[0]] = str(datetime.datetime.now().date())\n",
    "                    prompt = \"{ts} - UPDATE RECORD; Germline checking complete for {pool_id} with germline {germline}...\\n\".format(ts=str(datetime.datetime.now()), germline=full_sample_name_b, pool_id=pool_id)\n",
    "                    print(prompt)\n",
    "                else:\n",
    "                    line_dict[self.t_log_header_germline[0]] = \"NaN\"\n",
    "                    line_dict[self.t_log_header_date_germline[0]] = \"NaN\"\n",
    "                    prompt = \"{ts} - UPDATE RECORD; Germline checking NOT complete for {pool_id} with germline {germline}...\\n\".format(ts=str(datetime.datetime.now()), germline=full_sample_name_b, pool_id=pool_id)\n",
    "                    print(prompt)\n",
    "            except:\n",
    "                prompt = \"{ts} - UPDATE RECORD; Germline pool {pool_id}{suffix} with germline {germline} not created yet...\\n\".format(ts=str(datetime.datetime.now()), germline=full_sample_name_b, pool_id=pool_id, suffix=germline_pool_suffix)\n",
    "\n",
    "\n",
    "        #If data not been sent check if eligible\n",
    "        if(not uk_cloud):\n",
    "            if( check1 and check2 and germline ):\n",
    "                #¢ Hashed out while testing\n",
    "                #input_data = [pool_id, trial_id, sample_id_t, sample_id_b]\n",
    "                #cmd = [uk_cloud_transfer_script] + input_data\n",
    "                #subp.call(cmd)\n",
    "                uk_cloud = True\n",
    "                line_dict[self.t_log_header_ukcloud[0]] = \"True\"\n",
    "                line_dict[self.t_log_header_date_ukcloud[0]] = str(datetime.datetime.now().date())\n",
    "                prompt = \"{ts} - UPDATE RECORD; UKCloud transfer complete for {pool_id} with tumour {tumour} & germline {germline} pair...\\n\".format(ts=str(datetime.datetime.now()), tumour=full_sample_name_t,germline=full_sample_name_b, pool_id=pool_id)\n",
    "                print(prompt)\n",
    "            else:\n",
    "                line_dict[self.t_log_header_ukcloud[0]] = \"NaN\"\n",
    "                line_dict[self.t_log_header_date_ukcloud[0]] = \"NaN\"\n",
    "                prompt = \"{ts} - UPDATE RECORD; UKCloud transfer blocked due to somatic and germline sample checking NOT complete, please review log file for {pool_id} with tumour {tumour} & germline {germline} pair to investigate the cause...\\n\".format(ts=str(datetime.datetime.now()), tumour=full_sample_name_t,germline=full_sample_name_b, pool_id=pool_id)\n",
    "                print(prompt)\n",
    "                \n",
    "        return(line_dict)\n",
    "    \n",
    "    def check_fastq_ready(self, line_dict, match):\n",
    "        \n",
    "        #Instantiate variables\n",
    "        fastq_folder_root_path = UKCloud.config['file_system_objects']['fastq_analysis_folder_root_path']\n",
    "        fastq_ready_file = UKCloud.config['file_system_objects']['fastq_ready_file']\n",
    "        sample_id_t = match[self.t_log_header_tumour[1]]\n",
    "        sample_id_b = match[self.t_log_header_baseline[1]]\n",
    "        trial_id = match[self.t_log_header_trialID[1]]\n",
    "        full_sample_name_t = \"{sample_t}-{trialID}\".format(sample_t=sample_id_t, trialID=trial_id) \n",
    "        full_sample_name_b = \"{sample_b}-{trialID}\".format(sample_b=sample_id_b, trialID=trial_id)\n",
    "        uk_cloud_transfer_fastq_script = UKCloud.config['file_system_objects']['uk_cloud_transfer_fastq_script']\n",
    "        pool_id = line_dict[self.t_log_header_pool[0]]\n",
    "        \n",
    "        ## UKCloud\n",
    "        #==========\n",
    "        try:\n",
    "            uk_cloud = match[self.t_log_header_ukcloud[1]]\n",
    "            if(uk_cloud == \"NaN\"):\n",
    "                uk_cloud = False\n",
    "            elif(uk_cloud == \"True\"):\n",
    "                uk_cloud = True\n",
    "\n",
    "            line_dict[self.t_log_header_ukcloud[0]] = match[self.t_log_header_ukcloud[1]]\n",
    "            line_dict[self.t_log_header_date_ukcloud[0]] = match[self.t_log_header_date_ukcloud[1]]\n",
    "\n",
    "        except:\n",
    "            line_dict[self.t_log_header_ukcloud[0]] = \"NaN\"\n",
    "            line_dict[self.t_log_header_date_ukcloud[0]] = \"NaN\"\n",
    "            prompt = \"{ts} - NEW SAMPLES DETECTED; for {pool_id} with tumour {tumour} & germline {germline}; Queued for UKCloud transfer scan...\\n\".format(ts=str(datetime.datetime.now()), tumour=full_sample_name_t,germline=full_sample_name_b, pool_id=pool_id)\n",
    "            print(prompt)\n",
    "            uk_cloud = False\n",
    "        \n",
    "        #Check root folder exists\n",
    "        if(not uk_cloud):\n",
    "            if( os.path.exists(fastq_folder_root_path) ):\n",
    "                fastq_ready_file_abs_path = os.path.join(fastq_folder_root_path, pool_id, fastq_ready_file)\n",
    "\n",
    "                #Check fastq generation completed\n",
    "                if(os.path.exists(fastq_ready_file_abs_path)):\n",
    "                    \n",
    "                    ##If data not been sent check if eligible\n",
    "                    \n",
    "                    #¢X Hashed out while testing\n",
    "                    #input_data = [pool_id, trial_id, sample_id_t, sample_id_b]\n",
    "                    #cmd = [uk_cloud_transfer_fastq_script] + input_data\n",
    "                    #subp.call(cmd)\n",
    "                    uk_cloud = True\n",
    "                    line_dict[self.t_log_header_ukcloud[0]] = \"True\"\n",
    "                    line_dict[self.t_log_header_date_ukcloud[0]] = str(datetime.datetime.now().date())\n",
    "                    prompt = \"{ts} - UPDATE RECORD; UKCloud transfer complete for {pool_id} with tumour {tumour} & germline {germline} pair...\\n\".format(ts=str(datetime.datetime.now()), tumour=full_sample_name_t,germline=full_sample_name_b, pool_id=pool_id)\n",
    "                    print(prompt)    \n",
    "                else:\n",
    "                    \"{ts} INFO: Fastq generation for {pool} not complete, skipping untill next scheduled transfer\".format(ts=str(datetime.datetime.now().date()), pool=pool_id)\n",
    "            else:\n",
    "                print(\"{ts} - ERROR; {fastq_folder_root_path} does not exist, check that path in config file is correct.\".format(ts=str(datetime.datetime.now())), fastq_folder_root_path=fastq_folder_root_path)\n",
    "        else:\n",
    "            prompt = \"{ts} - UPDATE RECORD; UKCloud transfer blocked due to somatic and germline sample checking NOT complete, please review log file for {pool_id} with tumour {tumour} & germline {germline} pair to investigate the cause...\\n\".format(ts=str(datetime.datetime.now()), tumour=full_sample_name_t,germline=full_sample_name_b, pool_id=pool_id)\n",
    "            print(prompt)\n",
    "        \n",
    "        return(line_dict)\n",
    "              \n",
    "    ##Picks up the ready to transfer file created by write_dict_to_file.\n",
    "    # It prepares a subprocess call for each line that is scanned to be ready for\n",
    "    # transferring after confirming that checker 2 has been done. It will also update\n",
    "    def transfer_UKCloud(self):\n",
    "        ##Instantiate static variables\n",
    "        ready_to_transfer_file = UKCloud.config['log_files']['transfer_log']\n",
    "        output_path = UKCloud.config['file_system_objects']['logfile_dest_path']\n",
    "        legacy_field = \"FieldNotExistPrior2Update\" #Captures that some data did not have column prior to update\n",
    "        header = True\n",
    "\n",
    "        ##Set full paths for logfiles\n",
    "        ready_to_transfer_file =  os.path.join(output_path, ready_to_transfer_file)\n",
    "\n",
    "        ready_to_transfer_file_tmp = ready_to_transfer_file + \".tmp\"\n",
    "        new_line = \"\" ##Line to store updated data\n",
    "\n",
    "        ## Create output file with header if doesn't exist\n",
    "        #=================================================\n",
    "        if(not os.path.exists(ready_to_transfer_file_tmp)):\n",
    "            with open( os.path.join(ready_to_transfer_file_tmp), \"w\") as ready_IN:\n",
    "                ready_IN.write(self.transfer_log_header)\n",
    "\n",
    "        ## Scan each column for each line in transfer log and set boolean\n",
    "        # to be used to determine action\n",
    "        #===============================================================\n",
    "        with open(ready_to_transfer_file_tmp, \"a\") as ready_OUT:\n",
    "            with open(ready_to_transfer_file, \"r\") as ready_IN:\n",
    "                for line in ready_IN:\n",
    "                    line = line.rstrip()\n",
    "                    match = re.split(\"\\t\", line)\n",
    "                    line_dict = {}\n",
    "                    updated_line_dict = {}\n",
    "\n",
    "                    #Skip header\n",
    "                    if(header):\n",
    "                        for header_item in match:\n",
    "                            line_dict[header_item] = \"\"\n",
    "                        header = False\n",
    "                        continue\n",
    "\n",
    "                    #Read data and update line dict used to update transfer log\n",
    "                    line_dict[self.t_log_header_pool[0]] = match[self.t_log_header_pool[1]]\n",
    "                    line_dict[self.t_log_header_trialID[0]] = match[self.t_log_header_trialID[1]]\n",
    "                    line_dict[self.t_log_header_tumour[0]] = match[self.t_log_header_tumour[1]]\n",
    "                    line_dict[self.t_log_header_baseline[0]] = match[self.t_log_header_baseline[1]]\n",
    "                    \n",
    "                    #¢1 As this is a new field old data will not have it and error will need to be caught.\n",
    "                    try:\n",
    "                        line_dict[self.t_log_header_type[0]] = match[self.t_log_header_type[1]]\n",
    "                    except IndexError:\n",
    "                        line_dict[self.t_log_header_type[0]] = \"NaN\"\n",
    "                        \n",
    "                    # Samples already transferred to UKCloud can be skipped.\n",
    "                    #=======================================================\n",
    "                    try:\n",
    "                        line_dict[self.t_log_header_ukcloud[0]] = match[self.t_log_header_ukcloud[1]]\n",
    "                        if(line_dict[self.t_log_header_ukcloud[0]] == \"True\"):\n",
    "                            ready_OUT.write(line + \"\\n\")\n",
    "                            continue\n",
    "                    except:\n",
    "                        line_dict[self.t_log_header_ukcloud[0]] = \"NaN\"\n",
    "                        \n",
    "                    line_dict[self.t_log_header_check1[0]] = \"NaN\"\n",
    "                    line_dict[self.t_log_header_date_check1[0]] = \"NaN\"\n",
    "                    line_dict[self.t_log_header_check2[0]] = \"NaN\"\n",
    "                    line_dict[self.t_log_header_germline[0]] = \"NaN\"\n",
    "                    line_dict[self.t_log_header_date_germline[0]] = \"NaN\"\n",
    "                    line_dict[self.t_log_header_date_ukcloud[0]] = \"NaN\"\n",
    "       \n",
    "                    ## Scan & Update samples needing checker 1,2 authorised for both somatic and germline\n",
    "                    # - Panel-relapse = True\n",
    "                    #===========================================================================\n",
    "                    if(match[self.t_log_header_type[1]] == self.panel_relapse_dt \n",
    "                       or \n",
    "                       match[self.t_log_header_type[1]] == legacy_field):\n",
    "                        updated_line_dict = self.full_check_ck1ck2germ(line_dict, match)\n",
    "                        \n",
    "                    ## Scan & Update samples checking fastqs.\n",
    "                    #========================================\n",
    "                    elif(match[self.t_log_header_type[1]] == self.exome_dt):\n",
    "                        updated_line_dict = self.check_fastq_ready(line_dict, match)\n",
    "\n",
    "                    ## Update transfer log\n",
    "                    #======================\n",
    "                    new_tsv_line = self.write_dict2line(updated_line_dict)\n",
    "                    ready_OUT.write(new_tsv_line + \"\\n\")\n",
    "\n",
    "        #Overwrite old file with updated file\n",
    "        os.rename(ready_to_transfer_file_tmp, ready_to_transfer_file)\n",
    "\n",
    "def main(argv):\n",
    "    nr_arguments = 1 #Number of arguments required\n",
    "    script = \"UKCloud_transfer.py\" #Name of script\n",
    "\n",
    "    #Stores flags with arguments in opts (both flag in and input stored) and flags with no input in args\n",
    "#    try:\n",
    "#        opts, args = getopt.getopt(argv, \"c:h\", [\"config_file=\", \"help=\"])\n",
    "#    except getopt.GetoptError: # If no argument given\n",
    "#        print(script + ' -c <yaml_config>')\n",
    "#        sys.exit(2)\n",
    "#    #Loop through flags (opt) and arguments (arg) from opts. Store appropriately\n",
    "#    for opt, arg in opts:\n",
    "#        if opt == '-h':\n",
    "#            print(script + ' -c <YAML_config>')\n",
    "#            sys.exit(2)\n",
    "#        elif opt in (\"-c\", \"--config_file\"):\n",
    "#            config_file = arg\n",
    "\n",
    "#    #If only in or out given print usage and error else run script\n",
    "#    if len(argv)/2 < nr_arguments:\n",
    "#        print(\"You submitted \" + str(int(len(argv)/2)) + \" arguments, expected \" + str(nr_arguments))\n",
    "#        print('not enough arguments.... see below for run usage:')\n",
    "#        print(script + ' -c <yaml_config>')\n",
    "#        sys.exit(2)\n",
    "#    else:\n",
    "#        #¢Debug\n",
    "    if(True):\n",
    "        config_file = \"/Users/sjamal/Documents/Work/9.Git_scripts/UKCloud-transfer-StratMedPAED/config_file/config.yaml\"\n",
    "        \n",
    "        uk_cloud_obj = UKCloud(config_file)\n",
    "        glob_sample_sheet_dict = uk_cloud_obj.parse_sample_sheets()\n",
    "        uk_cloud_obj.write_dict_to_file(glob_sample_sheet_dict)\n",
    "        uk_cloud_obj.transfer_UKCloud()\n",
    "        print(\"-------------------------DONE-------------------------\")\n",
    "    else:\n",
    "        dummy = \"This if clause is just meant to maintain the indent after the comments are hashed out and if clause is removed.\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv[1:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
